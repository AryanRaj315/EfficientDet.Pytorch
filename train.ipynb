{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import Spine_dataset, get_augumentation, detection_collate\n",
    "from utils import EFFICIENTDET\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = None\n",
    "network = 'efficientdet-d0'\n",
    "num_epochs = 100\n",
    "batch_size = 6\n",
    "num_worker = 4\n",
    "num_classes = 1\n",
    "device = [0]\n",
    "grad_accumalation_steps = 1\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "save_folder = 'weights/'\n",
    "image_root = '../boostnet_labeldata/data/'\n",
    "csv_root = '../boostnet_labeldata/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_device(device):\n",
    "    n_gpu_use = len(device)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine, training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(\n",
    "            n_gpu_use, n_gpu))\n",
    "        n_gpu_use = n_gpu\n",
    "    list_ids = device\n",
    "    device = torch.device('cuda:{}'.format(\n",
    "        device[0]) if n_gpu_use > 0 else 'cpu')\n",
    "\n",
    "    return device, list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(model):\n",
    "    if type(model) == torch.nn.DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "if(resume is not None):\n",
    "    resume_path = str(resume)\n",
    "    print(\"Loading checkpoint: {} ...\".format(resume_path))\n",
    "    checkpoint = torch.load(\n",
    "        resume, map_location=lambda storage, loc: storage)\n",
    "    num_classes = checkpoint['num_classes']\n",
    "    network = checkpoint['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_train = pd.read_csv(csv_root+'training/landmarks.csv',header = None)\n",
    "filename_df_train = pd.read_csv(csv_root+'training/filenames.csv',header = None)\n",
    "boxes_df_train = pd.read_csv(csv_root+'training/train.csv')\n",
    "boxes_df_train.label = 0 # All boxes same class??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_test = pd.read_csv(csv_root+'test/landmarks.csv',header = None)\n",
    "filename_df_test = pd.read_csv(csv_root+'test/filenames.csv',header = None)\n",
    "boxes_df_test = pd.read_csv(csv_root+'test/test.csv')\n",
    "boxes_df_test.label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_train,corner_df_train,filename_df_train,transform=get_augumentation('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_test,corner_df_test,filename_df_test,transform=get_augumentation('test'),image_set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    print(idx ,images.shape, annotations.shape, corners.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientDet(num_classes=num_classes,\n",
    "                     network=network,\n",
    "                     W_bifpn=EFFICIENTDET[network]['W_bifpn'],\n",
    "                     D_bifpn=EFFICIENTDET[network]['D_bifpn'],\n",
    "                     D_class=EFFICIENTDET[network]['D_class'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(resume is not None):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "device, device_ids = prepare_device(device)\n",
    "model = model.to(device)\n",
    "if(len(device_ids) > 1):\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=3, verbose=True)\n",
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
    "    \n",
    "    start = time.time()\n",
    "    result = {}\n",
    "    total_loss = []\n",
    "    corner_losses = []\n",
    "    bbox_losses = []\n",
    "    cls_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    total_batches = len(train_dataloader)\n",
    "    tk0 = tqdm(train_dataloader, total=total_batches)\n",
    "    for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        annotations_bboxes = annotations_bboxes.to(device)\n",
    "        annotations_corners = annotations_corners.to(device)\n",
    "        classification, regression, corners, anchors = model(images)\n",
    "        classification_loss, regression_loss, corner_loss= criterion(\n",
    "            classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "        corner_loss = corner_loss.mean()\n",
    "        loss = classification_loss + regression_loss + corner_loss\n",
    "        if bool(loss == 0):\n",
    "            print('loss equal zero(0)')\n",
    "            continue\n",
    "        loss.backward()\n",
    "        if (idx+1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss.append(loss.item())\n",
    "        corner_losses.append(corner_loss.item())\n",
    "        bbox_losses.append(regression_loss.item())\n",
    "        cls_losses.append(classification_loss.item())\n",
    "        tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "    result = {\n",
    "        'time': time.time() - start,\n",
    "        'loss': np.mean(total_loss),\n",
    "        'corner_loss': np.mean(corner_losses),\n",
    "        'bbox_loss': np.mean(bbox_losses),\n",
    "        'cls_loss': np.mean(cls_losses)\n",
    "    }\n",
    "    for key, value in result.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value)) \n",
    "        \n",
    "    start = time.time()\n",
    "    result = {}\n",
    "    total_loss = []\n",
    "    corner_losses = []\n",
    "    bbox_losses = []\n",
    "    cls_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    total_batches = len(test_dataloader)\n",
    "    tk0 = tqdm(test_dataloader, total=total_batches)\n",
    "    for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        annotations_bboxes = annotations_bboxes.to(device)\n",
    "        annotations_corners = annotations_corners.to(device)\n",
    "        classification, regression, corners, anchors = model(images)\n",
    "        classification_loss, regression_loss, corner_loss= criterion(\n",
    "            classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "        corner_loss = corner_loss.mean()\n",
    "        loss = classification_loss + regression_loss + corner_loss\n",
    "        if bool(loss == 0):\n",
    "            print('loss equal zero(0)')\n",
    "            continue\n",
    "        if (idx+1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss.append(loss.item())\n",
    "        corner_losses.append(corner_loss.item())\n",
    "        bbox_losses.append(regression_loss.item())\n",
    "        cls_losses.append(classification_loss.item())\n",
    "        tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "    result = {\n",
    "        'time': time.time() - start,\n",
    "        'loss': np.mean(total_loss),\n",
    "        'corner_loss': np.mean(corner_losses),\n",
    "        'bbox_loss': np.mean(bbox_losses),\n",
    "        'cls_loss': np.mean(cls_losses)\n",
    "    }\n",
    "    for key, value in result.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value))\n",
    "    scheduler.step(np.mean(total_loss))\n",
    "\n",
    "    arch = type(model).__name__\n",
    "    state = {\n",
    "        'arch': arch,\n",
    "        'num_class': num_classes,\n",
    "        'network': network,\n",
    "        'state_dict': get_state_dict(model)\n",
    "    }\n",
    "    torch.save(\n",
    "        state, './weights/checkpoint_{}_{}.pth'.format(network, epoch))\n",
    "state = {\n",
    "    'arch': arch,\n",
    "    'num_class': num_classes,\n",
    "    'network': network,\n",
    "    'state_dict': get_state_dict(model)\n",
    "}\n",
    "torch.save(state, './weights/Final_{}.pth'.format(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
